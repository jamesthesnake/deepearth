# DeepEarth Coordinate System Conventions

This document outlines the internal coordinate system conventions used within the DeepEarth project, specifically for the 3D reconstruction modules (`deepearth.reconstruction`). Understanding these conventions is crucial for correctly interpreting poses, transforming points, and integrating with external libraries or data sources.

## 1. Internal World Coordinate System

The primary world coordinate system used *internally* for processing and initial point cloud generation (e.g., in `interactive_visualizer.py`, `geofusion_dataset.py`) is defined as follows:

*   **Origin:** The ECEF (Earth-Centered, Earth-Fixed) coordinate of the **first valid camera pose** loaded from the GeoFusion dataset (`geofusion.csv` matched with `log.ndjson`).
*   **Orientation:** The axes are aligned with the ECEF axes at the origin. While not strictly an ENU (East-North-Up) or NED (North-East-Down) frame centered at the origin, it serves as a consistent **local Cartesian frame** relative to the first camera.
*   **Scale:** Meters. Coordinates in this frame (`camera_centers_relative`, `points_world` generated by `unproject_depth`) represent metric distances relative to the first camera's ECEF position.

This relative system is chosen to:
*   Keep coordinate values relatively small, improving numerical stability compared to using raw ECEF values directly.
*   Simplify visualization by centering the scene near `[0, 0, 0]`.

## 2. Camera Coordinate System

We consistently use the **ARKit Camera Coordinate System** convention, as the depth data originates from ARKit:

*   **+X:** Points to the right in the camera's view.
*   **+Y:** Points upwards in the camera's view.
*   **-Z:** Points forward, into the scene (along the camera's viewing direction).

This is a right-handed coordinate system.

## 3. Pose Representation

Internally, poses are primarily handled using the **Camera-to-World (C2W)** convention relative to our defined world frame:

*   **`C_relative` (or `cam_center_world`):** A 3D vector representing the camera's origin (center of projection) in the *Internal World Coordinate System*.
*   **`R_cw` (or `cam_to_world_rot`):** A 3x3 rotation matrix that transforms a point's coordinates *from* the Camera Coordinate System *to* the Internal World Coordinate System. `P_world = R_cw @ P_camera + C_relative`.

The `GeoFusionDataset` calculates and stores `camera_centers_relative` and the **final `R_cw` (camera-to-world)** matrix required for this convention. It also pre-calculates the inverse **World-to-Camera (W2C)** pose (`w2c_rotation`, `w2c_translation`) for convenience.

## 4. Derivation from GeoFusion Input

The internal pose (`R_cw`, `C_relative`) is derived from the raw GeoFusion inputs through several steps performed mainly in `GeospatialConverter` and `GeoFusionDataset`:

1.  **Input:** Geodetic coordinates (Latitude, Longitude, Altitude) and orientation angles (Yaw, Pitch, Roll) from `geofusion.csv`.
2.  **YPR Interpretation:** Yaw, Pitch, Roll are assumed to follow the ZYX rotation sequence relative to a local **NED (North-East-Down)** frame, as per PIX4D documentation (Body frame: +X Front, +Y Right, +Z Down).
3.  **Body-to-NED Rotation (`R_ned_body`):** Calculated directly from YPR angles.
4.  **NED-to-ECEF Rotation (`R_ecef_ned`):** Calculated based on the geodetic latitude and longitude. Transforms NED axes to ECEF axes.
5.  **Body-to-ECEF Rotation (`R_ecef_body`):** `R_ecef_body = R_ecef_ned @ R_ned_body`. This matrix orients the *device body* frame in global ECEF coordinates.
6.  **Body-to-Camera Rotation (`R_body_cam`):** A *fixed* rotation matrix is applied to account for the difference between the device's body frame and the ARKit camera frame. This was **empirically determined** using `interactive_visualizer.py` to be approximately `Rz(90) = [[0, -1, 0], [1, 0, 0], [0, 0, 1]]`. This matrix transforms coordinates *from* the body frame *to* the camera frame.
    ```
    R_body_cam = torch.tensor([
        [0.0, -1.0, 0.0],
        [1.0,  0.0, 0.0],
        [0.0,  0.0, 1.0]
    ], ...)
    ```
7.  **Camera-to-World (ECEF) Rotation (`R_cw` / `R_ecef_cam`):** The final rotation orienting the *camera* frame in ECEF: `R_cw = R_ecef_cam = R_ecef_body @ R_body_cam`. This is stored in the dataset (after filtering matching views).
8.  **ECEF Position (`C_ecef`):** Calculated from geodetic coordinates.
9.  **Scene Origin:** Set to `C_ecef` of the first valid view.
10. **Relative Camera Center (`C_relative`):** `C_relative = C_ecef - SceneOrigin_ecef`. Stored in the dataset.

## 5. Conversion to Other Conventions

When interfacing with external libraries like `gsplat` or other rendering/processing tools, conversion might be necessary.

*   **World-to-Camera (W2C):** Many pipelines (including potentially `gsplat`, based on common NeRF practices) expect W2C poses. This is the inverse of our internal C2W representation:
    *   `R_wc = R_cw.T` (Transpose of the Camera-to-World rotation)
    *   `t_wc = -R_wc @ C_relative` (Translation vector)
    The `GeoFusionDataset` already calculates and provides `w2c_rotation` and `w2c_translation`.

*   **Other Conventions (OpenGL, COLMAP/OpenCV):**
    *   **OpenGL:** Often uses a coordinate system where the camera looks down -Z, +Y is Up, +X is Right. Converting from our ARKit-based camera frame (+X Right, +Y Up, -Z Forward) might require applying an additional transformation, often `diag(1, -1, -1)`, to the pose matrix *after* the W2C conversion, depending on the specific library's implementation.
    *   **COLMAP/OpenCV:** Typically uses +X Right, +Y Down, +Z Forward. Conversion would involve a different adjustment matrix (e.g., `diag(1, -1, -1)` applied differently or combined with the W2C transform).
    *   **Recommendation:** *Always* consult the documentation of the target library/pipeline to confirm its expected camera coordinate system and pose convention (C2W vs W2C, matrix layout). Apply the necessary basis change matrix multiplication.

## 6. Handling Other Input Sources

If integrating data from different sources (e.g., COLMAP reconstructions, other SLAM systems):

1.  Identify the coordinate system and pose convention of the new data source.
2.  Create a specific loader or adapter function for that source.
3.  Implement the necessary transformations within the adapter to convert the source data into the **DeepEarth Internal Convention** (`R_cw`, `C_relative` relative to the chosen scene origin, using the ARKit camera frame definition).
4.  Once converted to the internal standard, the data can be used consistently by downstream modules like `unproject_depth` or potentially `gsplat` (after converting to W2C if needed).

By standardizing internally and carefully managing conversions at the boundaries, we can maintain clarity and ensure correct geometric operations. 